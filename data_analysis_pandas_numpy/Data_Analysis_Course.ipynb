{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400455ed",
   "metadata": {},
   "source": [
    "## Установка библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087f3b1",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Установка необходимых библиотек в виртуальное окружение. Эти команды выполняются в терминале VS Code, а не в Python-скрипте. Рекомендуется использовать виртуальное окружение для изоляции зависимостей проекта. Библиотека openpyxl требуется для работы с Excel-файлами, requests — для загрузки данных по URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 1: Создание виртуального окружения (выполняется один раз)\n",
    "# python -m venv venv\n",
    "\n",
    "# Шаг 2: Активация окружения\n",
    "# Windows:\n",
    "# venv\\Scripts\\activate\n",
    "# macOS / Linux:\n",
    "# source venv/bin/activate\n",
    "\n",
    "# Шаг 3: Установка библиотек через pip\n",
    "# pip install pandas numpy openpyxl scikit-learn requests\n",
    "\n",
    "# Альтернативно: установка через conda (рекомендуется для data science)\n",
    "# conda install pandas numpy scikit-learn requests\n",
    "# pip install openpyxl  # openpyxl может не входить в conda по умолчанию\n",
    "\n",
    "# Проверка установки (в Python)\n",
    "# import pandas as pd\n",
    "# print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bfbf2",
   "metadata": {},
   "source": [
    "## Импорт модулей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa54b66",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Импорт всех необходимых библиотек в скрипт. Стандартные псевдонимы: pd для pandas, np для numpy. Эти библиотеки предоставляют инструменты для работы с данными, вычислениями и машинным обучением. Импорт выполняется один раз в начале скрипта и необходим для всех последующих операций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Все необходимые модули импортированы.\n",
    "# Теперь можно приступить к загрузке и анализу данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebdf6e",
   "metadata": {},
   "source": [
    "## Загрузка локального CSV-файла"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ce509",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Загрузка данных из локального CSV-файла 'SalesTarget.csv'. Файл должен находиться в той же директории, где запускается скрипт. Используется обработка ошибок на случай, если файл отсутствует. Кодировка по умолчанию — UTF-8, что подходит для большинства случаев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bffd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV-файл 'SalesTarget.csv' успешно загружен.\n",
      "Размер данных: 4603 строк, 5 столбцов\n",
      "Первые 3 строки:\n",
      "   row_id         Category  Order Date      Segment  Sales Target\n",
      "0       0  Office Supplies  2017-01-04     Consumer            15\n",
      "1       1  Office Supplies  2017-01-05  Home Office           300\n",
      "2       2  Office Supplies  2017-01-06     Consumer            21\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Загружаем CSV-файл\n",
    "    df_sales = pd.read_csv('SalesTarget.csv')\n",
    "    \n",
    "    # Выводим подтверждение и первые строки\n",
    "    print(\"✅ CSV-файл 'SalesTarget.csv' успешно загружен.\")\n",
    "    print(f\"Размер данных: {df_sales.shape[0]} строк, {df_sales.shape[1]} столбцов\")\n",
    "    print(\"Первые 3 строки:\")\n",
    "    print(df_sales.head(3))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Файл 'SalesTarget.csv' не найден в текущей директории.\")\n",
    "    print(\"Поместите файл в папку с проектом или укажите полный путь.\")\n",
    "    df_sales = pd.DataFrame()  # пустой датафрейм для продолжения без ошибок\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"❌ Файл 'SalesTarget.csv' пуст.\")\n",
    "    df_sales = pd.DataFrame()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Произошла ошибка при загрузке CSV: {e}\")\n",
    "    df_sales = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8c181",
   "metadata": {},
   "source": [
    "## Загрузка локального Excel-файла"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0882b",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Загрузка данных из Excel-файла 'online_retail_II.xlsx', содержащего два листа: 'Year 2010-2011' и 'Year 2009-2010'. Используется параметр sheet_name=None для загрузки всех листов в словарь. Библиотека openpyxl должна быть установлена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e04d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Ошибка при загрузке Excel: [Errno 13] Permission denied: 'online_retail_II.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Загружаем все листы Excel-файла в словарь\n",
    "    excel_sheets = pd.read_excel('online_retail_II.xlsx', sheet_name=None)\n",
    "    \n",
    "    # Извлекаем каждый лист в отдельный датафрейм\n",
    "    df_2010_2011 = excel_sheets['Year 2010-2011']\n",
    "    df_2009_2010 = excel_sheets['Year 2009-2010']\n",
    "    \n",
    "    # Выводим информацию о загруженных данных\n",
    "    print(\"✅ Excel-файл 'online_retail_II.xlsx' успешно загружен.\")\n",
    "    print(f\"Лист 'Year 2010-2011': {df_2010_2011.shape[0]} строк, {df_2010_2011.shape[1]} столбцов\")\n",
    "    print(f\"Лист 'Year 2009-2010': {df_2009_2010.shape[0]} строк, {df_2009_2010.shape[1]} столбцов\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Файл 'online_retail_II.xlsx' не найден.\")\n",
    "    print(\"Поместите файл в папку с проектом.\")\n",
    "    df_2010_2011 = pd.DataFrame()\n",
    "    df_2009_2010 = pd.DataFrame()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"❌ Лист не найден: {e}\")\n",
    "    print(\"Убедитесь, что названия листов точно соответствуют: 'Year 2010-2011', 'Year 2009-2010'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка при загрузке Excel: {e}\")\n",
    "    df_2010_2011 = pd.DataFrame()\n",
    "    df_2009_2010 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6f2c4",
   "metadata": {},
   "source": [
    "## Загрузка данных по URL (имитация)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067a280",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Имитация загрузки данных по URL. В реальности используется requests.get() для получения файла, а затем StringIO для передачи содержимого в pandas. Здесь используется фиктивный CSV для демонстрации логики. Полезно для работы с API или внешними источниками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64592d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Данные по URL (имитация) успешно загружены.\n",
      "Размер данных: 3 строк, 5 столбцов\n",
      "Пример данных:\n",
      "   row_id     Category  Order Date    Segment  Sales Target\n",
      "0       1  Electronics  2023-01-15  Wholesale          1500\n",
      "1       2     Clothing  2023-01-16     Retail           800\n",
      "2       3         Home  2023-01-17  Wholesale          1200\n"
     ]
    }
   ],
   "source": [
    "# Имитация URL-запроса\n",
    "url = 'https://example.com/data.csv'  # фиктивный URL\n",
    "\n",
    "try:\n",
    "    # В реальном сценарии:\n",
    "    # response = requests.get(url)\n",
    "    # if response.status_code == 200:\n",
    "    #     df_url = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    # Для демонстрации создадим пример данных\n",
    "    dummy_csv = \"row_id,Category,\\\"Order Date\\\",Segment,\\\"Sales Target\\\"\\n1,Electronics,2023-01-15,Wholesale,1500\\n2,Clothing,2023-01-16,Retail,800\\n3,Home,2023-01-17,Wholesale,1200\"\n",
    "    \n",
    "    df_url = pd.read_csv(StringIO(dummy_csv))\n",
    "    \n",
    "    print(\"✅ Данные по URL (имитация) успешно загружены.\")\n",
    "    print(f\"Размер данных: {df_url.shape[0]} строк, {df_url.shape[1]} столбцов\")\n",
    "    print(\"Пример данных:\")\n",
    "    print(df_url.head())\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ Ошибка сети при загрузке по URL: {e}\")\n",
    "    df_url = pd.DataFrame()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка при обработке данных по URL: {e}\")\n",
    "    df_url = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5abdf",
   "metadata": {},
   "source": [
    "## Загрузка данных из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17e943",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Демонстрация использования встроенных наборов данных из библиотеки scikit-learn. Функция make_regression генерирует синтетические данные для задачи регрессии. Это полезно для тестирования моделей и демонстрации возможностей библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4522cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Синтетические данные из sklearn успешно сгенерированы.\n",
      "Размер данных: 100 строк, 4 столбцов\n",
      "Первые 5 строк:\n",
      "   feature_0  feature_1  feature_2      target\n",
      "0  -0.792521   0.504987  -0.114736   13.510026\n",
      "1   0.280992  -0.208122  -0.622700  -18.777475\n",
      "2   0.791032   1.402794  -0.909387  111.265809\n",
      "3   0.625667  -1.070892  -0.857158  -77.989347\n",
      "4  -0.342715  -0.161286  -0.802277  -35.951738\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Генерация синтетических данных\n",
    "    X, y = make_regression(\n",
    "        n_samples=100,      # количество строк\n",
    "        n_features=3,       # количество признаков\n",
    "        noise=0.1,          # уровень шума\n",
    "        random_state=42     # для воспроизводимости\n",
    "    )\n",
    "    \n",
    "    # Преобразуем в DataFrame для удобства\n",
    "    df_sklearn = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    df_sklearn['target'] = y  # добавляем целевую переменную\n",
    "    \n",
    "    print(\"✅ Синтетические данные из sklearn успешно сгенерированы.\")\n",
    "    print(f\"Размер данных: {df_sklearn.shape[0]} строк, {df_sklearn.shape[1]} столбцов\")\n",
    "    print(\"Первые 5 строк:\")\n",
    "    print(df_sklearn.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка при генерации данных из sklearn: {e}\")\n",
    "    df_sklearn = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826d742",
   "metadata": {},
   "source": [
    "## Просмотр данных: первичный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89db9bf",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Базовые методы для первичного анализа данных: просмотр первых, последних и случайных строк, получение общей информации, статистики, типов данных и проверка пропусков. Это помогает понять структуру и качество данных перед предобработкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3baac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПЕРВИЧНЫЙ АНАЛИЗ ДАННЫХ ===\n",
      "\n",
      "🔹 Первые 3 строки:\n",
      "   row_id         Category  Order Date      Segment  Sales Target\n",
      "0       0  Office Supplies  2017-01-04     Consumer            15\n",
      "1       1  Office Supplies  2017-01-05  Home Office           300\n",
      "2       2  Office Supplies  2017-01-06     Consumer            21\n",
      "\n",
      "🔹 Последние 2 строки:\n",
      "      row_id         Category  Order Date    Segment  Sales Target\n",
      "4601    4601  Office Supplies  2020-12-31   Consumer            61\n",
      "4602    4602  Office Supplies  2020-12-31  Corporate            29\n",
      "\n",
      "🔹 Случайные 4 строки:\n",
      "      row_id         Category  Order Date      Segment  Sales Target\n",
      "1449    1449  Office Supplies  2018-07-31     Consumer            18\n",
      "643      643  Office Supplies  2017-10-06    Corporate           390\n",
      "3504    3504       Technology  2020-04-18  Home Office          2805\n",
      "4413    4413        Furniture  2020-11-25     Consumer           825\n",
      "\n",
      "🔹 Общая информация о датафрейме:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4603 entries, 0 to 4602\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   row_id        4603 non-null   int64 \n",
      " 1   Category      4603 non-null   object\n",
      " 2   Order Date    4603 non-null   object\n",
      " 3   Segment       4603 non-null   object\n",
      " 4   Sales Target  4603 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 179.9+ KB\n",
      "\n",
      "🔹 Статистическое описание числовых столбцов:\n",
      "            row_id  Sales Target\n",
      "count  4603.000000   4603.000000\n",
      "mean   2301.000000    486.726265\n",
      "std    1328.915974    985.114392\n",
      "min       0.000000      1.000000\n",
      "25%    1150.500000     56.000000\n",
      "50%    2301.000000    188.000000\n",
      "75%    3451.500000    552.000000\n",
      "max    4602.000000  25729.000000\n",
      "\n",
      "🔹 Типы данных каждого столбца:\n",
      "row_id           int64\n",
      "Category        object\n",
      "Order Date      object\n",
      "Segment         object\n",
      "Sales Target     int64\n",
      "dtype: object\n",
      "\n",
      "🔹 Размерность: 4603 строк × 5 столбцов\n",
      "\n",
      "🔹 Количество уникальных значений в каждом столбце:\n",
      "row_id          4603\n",
      "Category           3\n",
      "Order Date      1237\n",
      "Segment            3\n",
      "Sales Target    1342\n",
      "dtype: int64\n",
      "\n",
      "🔹 Количество пропущенных значений:\n",
      "row_id          0\n",
      "Category        0\n",
      "Order Date      0\n",
      "Segment         0\n",
      "Sales Target    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Выбираем датасет для анализа (например, df_2009_2010)\n",
    "df = df_2009_2010.copy() if not df_2009_2010.empty else df_sales\n",
    "\n",
    "if df.empty:\n",
    "    print(\"Нет данных для анализа.\")\n",
    "else:\n",
    "    print(\"\"\"=== ПЕРВИЧНЫЙ АНАЛИЗ ДАННЫХ ===\"\"\")\n",
    "    \n",
    "    # Просмотр структуры\n",
    "    print(\"\\n🔹 Первые 3 строки:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    print(\"\\n🔹 Последние 2 строки:\")\n",
    "    print(df.tail(2))\n",
    "    \n",
    "    print(\"\\n🔹 Случайные 4 строки:\")\n",
    "    print(df.sample(4))\n",
    "    \n",
    "    # Общая информация\n",
    "    print(\"\\n🔹 Общая информация о датафрейме:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\n🔹 Статистическое описание числовых столбцов:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n🔹 Типы данных каждого столбца:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n🔹 Размерность: {df.shape[0]} строк × {df.shape[1]} столбцов\")\n",
    "    \n",
    "    print(\"\\n🔹 Количество уникальных значений в каждом столбце:\")\n",
    "    print(df.nunique())\n",
    "    \n",
    "    print(\"\\n🔹 Количество пропущенных значений:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Проверка ключевых полей\n",
    "    if 'InvoiceDate' in df.columns:\n",
    "        print(f\"\\n🔹 Тип 'InvoiceDate': {df['InvoiceDate'].dtype}\")\n",
    "    if 'Quantity' in df.columns:\n",
    "        print(f\"🔹 Тип 'Quantity': {df['Quantity'].dtype}\")\n",
    "    if 'UnitPrice' in df.columns:\n",
    "        print(f\"🔹 Тип 'UnitPrice': {df['UnitPrice'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6954d5",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96244f",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Очистка и подготовка 'грязного' датасета (на примере online_retail_II). Включает обработку пропусков, дубликатов, приведение типов, удаление выбросов и извлечение признаков из даты. Каждый шаг критически важен для получения качественных результатов анализа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94125533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет данных для предобработки.\n"
     ]
    }
   ],
   "source": [
    "# Работаем с df_2009_2010 как с основным датасетом\n",
    "if df_2009_2010.empty:\n",
    "    print(\"Нет данных для предобработки.\")\n",
    "else:\n",
    "    df_clean = df_2009_2010.copy()\n",
    "    initial_shape = df_clean.shape\n",
    "    \n",
    "    print(\"\"\"=== ПРЕДОБРАБОТКА ДАННЫХ ===\"\"\")\n",
    "    print(f\"Исходный размер: {initial_shape[0]} строк\")\n",
    "    \n",
    "    # 1. Обработка пропущенных значений\n",
    "    df_clean.dropna(subset=['CustomerID', 'Description'], inplace=True)\n",
    "    print(\"✅ Удалены строки с пропусками в CustomerID и Description\")\n",
    "    \n",
    "    # 2. Удаление дубликатов\n",
    "    duplicates_before = initial_shape[0] - df_clean.shape[0]\n",
    "    df_clean.drop_duplicates(inplace=True)\n",
    "    duplicates_after = df_clean.duplicated().sum()\n",
    "    print(f\"✅ Удалено дубликатов: {df_clean.shape[0] - duplicates_after}\")\n",
    "    \n",
    "    # 3. Приведение типов\n",
    "    # Дата\n",
    "    if 'InvoiceDate' in df_clean.columns:\n",
    "        df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'], errors='coerce')\n",
    "        print(\"✅ Столбец 'InvoiceDate' преобразован в datetime\")\n",
    "    \n",
    "    # Числовые поля\n",
    "    for col in ['Quantity', 'UnitPrice']:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    df_clean.dropna(subset=['Quantity', 'UnitPrice'], inplace=True)\n",
    "    print(\"✅ Числовые столбцы приведены к корректным типам\")\n",
    "    \n",
    "    # 4. Очистка от выбросов\n",
    "    outliers = df_clean[\n",
    "        (df_clean['Quantity'] <= 0) | \n",
    "        (df_clean['UnitPrice'] <= 0)\n",
    "    ].shape[0]\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['Quantity'] > 0) & \n",
    "        (df_clean['UnitPrice'] > 0)\n",
    "    ]\n",
    "    print(f\"✅ Удалено выбросов (Quantity≤0 или Price≤0): {outliers}\")\n",
    "    \n",
    "    # 5. Извлечение признаков из даты\n",
    "    if 'InvoiceDate' in df_clean.columns:\n",
    "        df_clean['Year'] = df_clean['InvoiceDate'].dt.year\n",
    "        df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
    "        df_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.dayofweek\n",
    "        df_clean['Date'] = df_clean['InvoiceDate'].dt.date\n",
    "        print(\"✅ Извлечены признаки из даты: Year, Month, DayOfWeek, Date\")\n",
    "    \n",
    "    final_shape = df_clean.shape\n",
    "    print(f\"✅ Предобработка завершена. Итоговый размер: {final_shape[0]} строк\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b0342",
   "metadata": {},
   "source": [
    "## Трансформации данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898181ac",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Создание новых столбцов, удаление ненужных, переименование, сортировка и фильтрация данных. Эти операции позволяют адаптировать данные под конкретные задачи анализа. Все операции выполняются с использованием векторизованных функций pandas для максимальной производительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87251bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет данных для трансформации.\n"
     ]
    }
   ],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    print(\"\"\"=== ТРАНСФОРМАЦИИ ДАННЫХ ===\"\"\")\n",
    "    \n",
    "    # 1. Создание нового столбца\n",
    "    df_clean['Total'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "    print(\"✅ Создан столбец 'Total' = Quantity × UnitPrice\")\n",
    "    \n",
    "    # 2. Удаление столбцов\n",
    "    # Пример: удаляем InvoiceDate, если есть Date\n",
    "    # if 'InvoiceDate' in df_clean.columns:\n",
    "    #     df_clean.drop(columns=['InvoiceDate'], inplace=True)\n",
    "    #     print(\"✅ Удалён столбец 'InvoiceDate'\")\n",
    "    \n",
    "    # 3. Переименование столбцов\n",
    "    # Пример:\n",
    "    # df_clean.rename(columns={'UnitPrice': 'Price'}, inplace=True)\n",
    "    # print(\"✅ Столбец 'UnitPrice' переименован в 'Price'\")\n",
    "    \n",
    "    # 4. Сортировка\n",
    "    # Сортировка по дате и сумме\n",
    "    # if 'InvoiceDate' in df_clean.columns:\n",
    "    #     df_clean.sort_values(by=['InvoiceDate', 'Total'], ascending=[True, False], inplace=True)\n",
    "    #     print(\"✅ Данные отсортированы по дате и общей сумме\")\n",
    "    \n",
    "    # 5. Фильтрация\n",
    "    # Пример 1: заказы дороже 100\n",
    "    # df_filtered = df_clean[df_clean['Total'] > 100]\n",
    "    # print(f\"✅ Отфильтровано: {df_filtered.shape[0]} заказов дороже 100\")\n",
    "    \n",
    "    # Пример 2: только определённые страны\n",
    "    # countries = ['United Kingdom', 'France', 'Germany']\n",
    "    # df_filtered = df_clean[df_clean['Country'].isin(countries)]\n",
    "    # print(f\"✅ Отфильтровано: {df_filtered.shape[0]} строк из {len(countries)} стран\")\n",
    "    \n",
    "    # Пример 3: использование query\n",
    "    # df_filtered = df_clean.query('Quantity > 10 and Country == \"France\"')\n",
    "    # print(f\"✅ Отфильтровано по условию: {df_filtered.shape[0]} строк\")\n",
    "    \n",
    "    print(\"✅ Трансформации выполнены.\")\n",
    "else:\n",
    "    print(\"Нет данных для трансформации.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899df0c",
   "metadata": {},
   "source": [
    "## Группировка и агрегация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756111a",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Группировка данных по одному или нескольким признакам с последующей агрегацией (сумма, среднее и т.д.). Создание сводных таблиц и кросс-таблиц для анализа многомерных данных. Эти методы позволяют получить бизнес-инсайты из больших объёмов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faaab15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет данных для группировки.\n"
     ]
    }
   ],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    print(\"\"\"=== ГРУППИРОВКА И АГРЕГАЦИЯ ===\"\"\")\n",
    "    \n",
    "    # 1. Группировка по стране\n",
    "    sales_by_country = df_clean.groupby('Country')['Total'].agg([\n",
    "        'sum', 'mean', 'count', 'std'\n",
    "    ]).round(2)\n",
    "    sales_by_country.rename(columns={\n",
    "        'sum': 'TotalSales',\n",
    "        'mean': 'AvgOrder',\n",
    "        'count': 'NumOrders',\n",
    "        'std': 'SalesStd'\n",
    "    }, inplace=True)\n",
    "    sales_by_country = sales_by_country.sort_values('TotalSales', ascending=False)\n",
    "    \n",
    "    print(\"📊 Продажи по странам (топ-5):\")\n",
    "    print(sales_by_country.head())\n",
    "    \n",
    "    # 2. Группировка по стране и году\n",
    "    if 'Year' in df_clean.columns:\n",
    "        sales_by_country_year = df_clean.groupby(['Country', 'Year'])['Total'].agg(\n",
    "            TotalSales=('sum'),\n",
    "            AvgOrder=('mean')\n",
    "        ).round(2)\n",
    "        \n",
    "        print(\"\\n📊 Продажи по странам и годам (топ-6):\")\n",
    "        print(sales_by_country_year.head(6))\n",
    "    \n",
    "    # 3. Сводная таблица: средний чек по странам и месяцам\n",
    "    if 'Month' in df_clean.columns:\n",
    "        pivot_sales = df_clean.pivot_table(\n",
    "            values='Total',\n",
    "            index='Country',\n",
    "            columns='Month',\n",
    "            aggfunc='mean',\n",
    "            fill_value=0,\n",
    "            margins=True,\n",
    "            margins_name='Average'\n",
    "        ).round(2)\n",
    "        \n",
    "        print(\"\\n📊 Средний чек по месяцам (первые 5 стран):\")\n",
    "        print(pivot_sales.head())\n",
    "    \n",
    "    # 4. Кросс-таблица: распределение заказов по дням недели и месяцам\n",
    "    if 'DayOfWeek' in df_clean.columns:\n",
    "        cross_table = pd.crosstab(\n",
    "            df_clean['DayOfWeek'],\n",
    "            df_clean['Month'],\n",
    "            normalize='index'\n",
    "        ).round(3)\n",
    "        \n",
    "        print(\"\\n📊 Распределение заказов по дням недели и месяцам:\")\n",
    "        print(cross_table.head())\n",
    "    \n",
    "    print(\"✅ Группировка и агрегация завершены.\")\n",
    "else:\n",
    "    print(\"Нет данных для группировки.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28df51a",
   "metadata": {},
   "source": [
    "## Сохранение результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7e9f3",
   "metadata": {},
   "source": [
    "**Описание:**\n",
    "\n",
    "Сохранение предобработанного датасета в JSON с разными форматами (records, index, split, values). Также сохраняется сводная таблица в CSV для демонстрации. Все файлы сохраняются в текущей директории. Формат JSON с параметром force_ascii=False позволяет корректно сохранять кириллицу и другие символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b1f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет данных для сохранения.\n"
     ]
    }
   ],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    print(\"\"\"=== СОХРАНЕНИЕ РЕЗУЛЬТАТОВ ===\"\"\")\n",
    "    \n",
    "    # 1. Сохранение в JSON с разными форматами\n",
    "    formats = ['records', 'index', 'split', 'values']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        filename = f'cleaned_data_{fmt}.json'\n",
    "        try:\n",
    "            df_clean.to_json(\n",
    "                filename,\n",
    "                orient=fmt,\n",
    "                indent=2 if fmt != 'values' else None,\n",
    "                force_ascii=False\n",
    "            )\n",
    "            print(f\"✅ Сохранено: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при сохранении {filename}: {e}\")\n",
    "    \n",
    "    # 2. Сохранение сводной таблицы в CSV\n",
    "    if 'pivot_sales' in locals():\n",
    "        try:\n",
    "            pivot_sales.to_csv('pivot_sales_by_month.csv', encoding='utf-8-sig')\n",
    "            print(\"✅ Сохранено: pivot_sales_by_month.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при сохранении CSV: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️  Сводная таблица не создана, пропускаем сохранение.\")\n",
    "    \n",
    "    print(\"\\n🎉 Все результаты успешно сохранены в текущей директории.\")\n",
    "else:\n",
    "    print(\"Нет данных для сохранения.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
