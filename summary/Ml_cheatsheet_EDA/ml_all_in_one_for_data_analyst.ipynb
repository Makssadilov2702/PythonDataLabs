{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning для аналитика — единый конспект\n",
        "\n",
        "В этом ноутбуке собраны основные темы ML для аналитика:\n",
        "1) **Основы ML** (train/test, кодирование, масштабирование, базовые метрики)  \n",
        "2) **Регрессия** (линейная/множественная, полиномиальная, метрики)  \n",
        "3) **Классификация** (логистическая регрессия, дерево решений, Random Forest, метрики)  \n",
        "4) **Кластеризация** (k-means, иерархическая, DBSCAN, Elbow, Silhouette)  \n",
        "5) **Бустинг и ансамбли** (идея, Gradient Boosting; XGBoost/LightGBM/CatBoost — опционально)\n",
        "\n",
        "⚙️ Подход: максимум понятности, минимум магии. Код снабжён комментариями **к каждой строке**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Основы ML: подготовка данных и базовая модель\n",
        "**Что покажем:**\n",
        "- Разделение данных на train/test с `stratify`\n",
        "- Кодирование категориальных признаков (One-Hot)\n",
        "- Масштабирование числовых признаков (StandardScaler)\n",
        "- Пайплайн (Pipeline) = предобработка + модель\n",
        "- Метрики классификации (Accuracy/Precision/Recall/F1)\n",
        "\n",
        "Мы создадим маленький искусственный датасет: `city` (категория), `age` (число), `bought` (класс 0/1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # импортируем pandas для работы с таблицами\n",
        "from sklearn.model_selection import train_test_split  # функция для разбиения на train/test\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler  # кодирование категориальных, масштабирование числовых\n",
        "from sklearn.compose import ColumnTransformer  # позволяет применять разные трансформации к разным колонкам\n",
        "from sklearn.pipeline import Pipeline  # объединяет шаги предобработки и модель в одну цепочку\n",
        "from sklearn.linear_model import LogisticRegression  # базовая модель бинарной классификации\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # метрики качества\n",
        "\n",
        "# создаём небольшой датафрейм с категориальным и числовым признаком и целевой переменной\n",
        "df = pd.DataFrame({\n",
        "    'city': ['Moscow', 'SPB', 'Kazan', 'Moscow', 'Kazan', 'SPB', 'Moscow', 'SPB'],  # города (категория)\n",
        "    'age': [22, 35, 41, 28, 50, 31, 23, 47],  # возраст (число)\n",
        "    'bought': [0, 1, 1, 0, 1, 0, 0, 1]  # целевой класс: купил (1) / нет (0)\n",
        "})\n",
        "\n",
        "X = df[['city', 'age']]  # матрица признаков: берём столбцы city и age\n",
        "y = df['bought']  # целевая переменная (Series)\n",
        "\n",
        "cat_features = ['city']  # список категориальных колонок\n",
        "num_features = ['age']   # список числовых колонок\n",
        "\n",
        "# ColumnTransformer применяет One-Hot к city и StandardScaler к age\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),  # OHE: векторизуем город\n",
        "        ('num', StandardScaler(), num_features)  # стандартизируем возраст (ср.=0, std=1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pipeline: сначала preprocess, потом обучаем LogisticRegression\n",
        "clf = Pipeline(steps=[\n",
        "    ('prep', preprocess),  # шаг предобработки признаков\n",
        "    ('model', LogisticRegression(max_iter=1000, random_state=42))  # сама модель\n",
        "])\n",
        "\n",
        "# train_test_split с stratify=y — распределение классов сохраняется и в train, и в test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y  # 25% данных в тест, фиксируем случайность и стратифицируем\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)  # обучаем пайплайн на обучающих данных\n",
        "y_pred = clf.predict(X_test)  # делаем предсказания классов на тесте\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))   # доля верных ответов\n",
        "print('Precision:', precision_score(y_test, y_pred)) # сколько из предсказанных 1 реально 1\n",
        "print('Recall:', recall_score(y_test, y_pred))       # какую долю всех реальных 1 нашли\n",
        "print('F1:', f1_score(y_test, y_pred))               # гармоническое среднее precision и recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задания (Основы)\n",
        "1. Добавьте в датафрейм новый категориальный признак (например, `channel` = ['online','offline']) и обновите пайплайн.  \n",
        "2. Поменяйте тестовый размер на 0.4 и посмотрите, как меняются метрики.  \n",
        "3. Попробуйте удалить `StandardScaler` — изменится ли качество? Почему?  \n",
        "4. Добавьте в `LogisticRegression` параметр `C` (например, 0.5 и 2.0) и сравните метрики.  \n",
        "5. Выведите `clf.named_steps['prep'].get_feature_names_out()` и посмотрите, какие фичи получились после OHE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Регрессия: линейная и полиномиальная\n",
        "**Что покажем:**\n",
        "- Линейная регрессия на реальном датасете `load_diabetes`\n",
        "- Полиномиальная регрессия на синтетических данных\n",
        "- Метрики: MAE, MSE, R²\n",
        "\n",
        "Почему `load_diabetes`? Это готовый датасет из sklearn, не требует загрузки из интернета и часто используется как учебный пример для регрессии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 42.794094679599944\n",
            "MSE: 2900.1936284934827\n",
            "R^2: 0.45260276297191926\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  # для численных операций\n",
        "from sklearn.datasets import load_diabetes  # датасет для регрессии (прогноз показателя болезни)\n",
        "from sklearn.linear_model import LinearRegression  # линейная регрессия\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # метрики регрессии\n",
        "from sklearn.model_selection import train_test_split  # разбиение на train/test\n",
        "from sklearn.preprocessing import StandardScaler  # масштабирование признаков\n",
        "from sklearn.pipeline import Pipeline  # конвейер из шагов\n",
        "\n",
        "diab = load_diabetes()  # загружаем датасет (X — признаки, y — целевая числовая)\n",
        "X = diab.data           # матрица признаков (уже числовые, стандартизованные диапазоны)\n",
        "y = diab.target         # целевая переменная (число)\n",
        "\n",
        "# разбиение 80/20, фиксируем random_state для воспроизводимости\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# конвейер: масштабирование (дополнительно) + линейная регрессия\n",
        "reg = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),  # масштабируем признаки\n",
        "    ('linreg', LinearRegression()) # обучаем линейную регрессию\n",
        "])\n",
        "\n",
        "reg.fit(X_train, y_train)   # обучаем модель на train\n",
        "y_pred = reg.predict(X_test) # предсказываем для test\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_test, y_pred))  # средняя абсолютная ошибка (в ед. y)\n",
        "print('MSE:', mean_squared_error(y_test, y_pred))   # средний квадрат ошибки (чувствителен к выбросам)\n",
        "print('R^2:', r2_score(y_test, y_pred))             # доля объяснённой дисперсии (чем ближе к 1, тем лучше)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Полиномиальная регрессия (синтетические данные)\n",
        "Смоделируем зависимость \\(y = 0.5x^2 + 1.0x + 2.0 + \\text{шум}\\). Используем `PolynomialFeatures(degree=2)` для добавления \\(x^2\\)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (poly): 0.3426751184738956\n",
            "MSE (poly): 0.1809042206645824\n",
            "R^2 (poly): 0.9640457572787543\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures  # генерация полиномиальных признаков\n",
        "\n",
        "rng = np.random.default_rng(42)          # генератор случайностей для воспроизводимости\n",
        "x = np.linspace(-3, 3, 150).reshape(-1, 1)  # создаём столбец значений x от -3 до 3\n",
        "y_true = 0.5 * x[:, 0]**2 + 1.0 * x[:, 0] + 2.0  # истинная квадратичная зависимость (без шума)\n",
        "y = y_true + rng.normal(0, 0.5, size=len(x))     # добавляем шум с нормальным распределением\n",
        "\n",
        "poly_reg = Pipeline(steps=[\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # генерируем признаки x и x^2\n",
        "    ('linreg', LinearRegression())                               # обучаем линейную регрессию на расширенных фичах\n",
        "])\n",
        "\n",
        "poly_reg.fit(x, y)               # обучаем модель на всех точках (для простоты)\n",
        "y_pred_poly = poly_reg.predict(x) # предсказываем на тех же x (оценим, как модель подстроилась)\n",
        "\n",
        "print('MAE (poly):', mean_absolute_error(y, y_pred_poly))  # метрики ошибки\n",
        "print('MSE (poly):', mean_squared_error(y, y_pred_poly))\n",
        "print('R^2 (poly):', r2_score(y, y_pred_poly))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задания (Регрессия)\n",
        "1. Попробуйте в `PolynomialFeatures` поменять `degree` на 3 и 4. Что происходит с R²?  \n",
        "2. В `load_diabetes` уберите `StandardScaler`. Изменятся ли метрики? Почему?  \n",
        "3. Добавьте в линейную регрессию регуляризацию: замените `LinearRegression` на `Ridge` (или `Lasso`).  \n",
        "4. Посчитайте `RMSE = sqrt(MSE)` и сравните с `MAE`. Чем эти метрики отличаются по смыслу?  \n",
        "5. Разбейте синтетические данные на train/test (например, 70/30) и сравните качество на train и test (переобучение?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Классификация: логистическая регрессия, дерево, случайный лес\n",
        "**Что покажем:**\n",
        "- Датасет `load_breast_cancer` (бинарная классификация)\n",
        "- Три модели: Logistic Regression, Decision Tree, Random Forest\n",
        "- Метрики: Accuracy, Precision, Recall, F1, ROC-AUC\n",
        "\n",
        "Важно: используем `stratify=y` при разбиении, чтобы классы были представлены корректно и в train, и в test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Logistic Regression ---\n",
            "Accuracy: 0.9824561403508771\n",
            "Precision: 0.9861111111111112\n",
            "Recall: 0.9861111111111112\n",
            "F1: 0.9861111111111112\n",
            "ROC-AUC: 0.9953703703703703\n",
            "\n",
            "--- Decision Tree ---\n",
            "Accuracy: 0.9210526315789473\n",
            "Precision: 0.9565217391304348\n",
            "Recall: 0.9166666666666666\n",
            "F1: 0.9361702127659575\n",
            "ROC-AUC: 0.9163359788359788\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.956140350877193\n",
            "Precision: 0.958904109589041\n",
            "Recall: 0.9722222222222222\n",
            "F1: 0.9655172413793104\n",
            "ROC-AUC: 0.9930555555555556\n",
            "\n",
            "Classification report (RF):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.95      0.93      0.94        42\n",
            "      benign       0.96      0.97      0.97        72\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer  # встроенный датасет для бинарной классификации\n",
        "from sklearn.tree import DecisionTreeClassifier   # дерево решений\n",
        "from sklearn.ensemble import RandomForestClassifier  # случайный лес (ансамбль деревьев)\n",
        "from sklearn.linear_model import LogisticRegression  # логистическая регрессия\n",
        "from sklearn.metrics import roc_auc_score, classification_report  # ROC-AUC и текстовый отчёт по классам\n",
        "\n",
        "data = load_breast_cancer()  # загружаем признаки и цель\n",
        "X = data.data                # матрица числовых признаков\n",
        "y = data.target              # целевая (0/1)\n",
        "\n",
        "# делим на train/test с сохранением баланса классов\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1) Логистическая регрессия с масштабированием (часто помогает для LR)\n",
        "logreg = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),  # масштабируем фичи\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))  # обучаем LR\n",
        "])\n",
        "logreg.fit(X_train, y_train)               # обучение\n",
        "pred_lr = logreg.predict(X_test)           # предсказания классов\n",
        "proba_lr = logreg.predict_proba(X_test)[:, 1]  # вероятности класса 1 (нужны для ROC-AUC)\n",
        "\n",
        "print('--- Logistic Regression ---')\n",
        "print('Accuracy:', accuracy_score(y_test, pred_lr))\n",
        "print('Precision:', precision_score(y_test, pred_lr))\n",
        "print('Recall:', recall_score(y_test, pred_lr))\n",
        "print('F1:', f1_score(y_test, pred_lr))\n",
        "print('ROC-AUC:', roc_auc_score(y_test, proba_lr))\n",
        "\n",
        "# 2) Дерево решений\n",
        "tree = DecisionTreeClassifier(max_depth=5, random_state=42)  # ограничим глубину для устойчивости\n",
        "tree.fit(X_train, y_train)               # обучение\n",
        "pred_tree = tree.predict(X_test)         # предсказания классов\n",
        "proba_tree = tree.predict_proba(X_test)[:, 1]  # вероятности класса 1\n",
        "\n",
        "print('\\n--- Decision Tree ---')\n",
        "print('Accuracy:', accuracy_score(y_test, pred_tree))\n",
        "print('Precision:', precision_score(y_test, pred_tree))\n",
        "print('Recall:', recall_score(y_test, pred_tree))\n",
        "print('F1:', f1_score(y_test, pred_tree))\n",
        "print('ROC-AUC:', roc_auc_score(y_test, proba_tree))\n",
        "\n",
        "# 3) Случайный лес\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)  # ансамбль из 200 деревьев\n",
        "rf.fit(X_train, y_train)                 # обучение\n",
        "pred_rf = rf.predict(X_test)             # предсказания классов\n",
        "proba_rf = rf.predict_proba(X_test)[:, 1]  # вероятности класса 1\n",
        "\n",
        "print('\\n--- Random Forest ---')\n",
        "print('Accuracy:', accuracy_score(y_test, pred_rf))\n",
        "print('Precision:', precision_score(y_test, pred_rf))\n",
        "print('Recall:', recall_score(y_test, pred_rf))\n",
        "print('F1:', f1_score(y_test, pred_rf))\n",
        "print('ROC-AUC:', roc_auc_score(y_test, proba_rf))\n",
        "\n",
        "print('\\nClassification report (RF):\\n', classification_report(y_test, pred_rf, target_names=data.target_names))  # подробный отчёт\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задания (Классификация)\n",
        "1. В `DecisionTreeClassifier` измените `max_depth` (например, 3 и 10) и сравните метрики.  \n",
        "2. В `RandomForestClassifier` измените `n_estimators` (например, 50 и 500). Как это влияет на качество и время?  \n",
        "3. В `LogisticRegression` попробуйте `C=0.5` и `C=2.0`. Как изменятся Precision/Recall?  \n",
        "4. Посчитайте `roc_auc_score` для дерева и леса и сравните с LR.  \n",
        "5. Попробуйте `class_weight='balanced'` в одной из моделей и посмотрите, как меняются метрики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Кластеризация: k-means, иерархическая, DBSCAN\n",
        "**Что покажем:**\n",
        "- Синтетические данные с 3 кластерами (`make_blobs`)\n",
        "- KMeans, AgglomerativeClustering, DBSCAN\n",
        "- Как выбрать число кластеров: Elbow (WCSS) и Silhouette\n",
        "\n",
        "Вместо графиков выведем численные значения (под визуализацию у тебя уже есть свой ноут)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans: silhouette (k=3) = 0.8480303059596955\n",
            "Agglomerative: silhouette (k=3) = 0.8480303059596955\n",
            "DBSCAN: silhouette = 0.7823715398620228\n",
            "Elbow WCSS (k, WCSS): [(2, 5763.464789461435), (3, 566.8595511244131), (4, 496.4279529785782), (5, 427.1286535498067), (6, 375.0331583497969), (7, 308.19836610418247), (8, 272.405731498748), (9, 234.28072349591127)]\n",
            "Silhouette (k, score): [(2, 0.7049437310743717), (3, 0.8480303059596955), (4, 0.6636976714243523), (5, 0.49012744554094295), (6, 0.5168058221331912), (7, 0.358030586983425), (8, 0.3625632360122803), (9, 0.37134027555333055)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_blobs  # генерация синтетических кластеров\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN  # алгоритмы кластеризации\n",
        "from sklearn.metrics import silhouette_score  # коэффициент силуэта для оценки качества кластеров\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)  # генерируем 300 точек в 3 кластерах\n",
        "\n",
        "# 4.1 KMeans (k=3)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)  # задаём k=3 и число инициализаций\n",
        "labels_km = kmeans.fit_predict(X)  # обучаем и получаем метки кластеров сразу\n",
        "sil_km = silhouette_score(X, labels_km)  # считаем силуэт для оценки разделения\n",
        "print('KMeans: silhouette (k=3) =', sil_km)\n",
        "\n",
        "# 4.2 Иерархическая кластеризация (агломеративная)\n",
        "agg = AgglomerativeClustering(n_clusters=3)  # тоже хотим 3 кластера\n",
        "labels_agg = agg.fit_predict(X)             # обучаем и получаем метки\n",
        "sil_agg = silhouette_score(X, labels_agg)   # силуэт\n",
        "print('Agglomerative: silhouette (k=3) =', sil_agg)\n",
        "\n",
        "# 4.3 DBSCAN (по плотности)\n",
        "db = DBSCAN(eps=0.9, min_samples=5)  # радиус соседства и минимум точек в окрестности\n",
        "labels_db = db.fit_predict(X)        # метки кластеров, -1 означает шум/выбросы\n",
        "\n",
        "# силуэт для DBSCAN считаем, только если получилось >1 кластера и не все точки шум\n",
        "unique_db = set(labels_db)\n",
        "if len(unique_db - {-1}) >= 2:  # как минимум 2 реальных кластера\n",
        "    sil_db = silhouette_score(X, labels_db)\n",
        "    print('DBSCAN: silhouette =', sil_db)\n",
        "else:\n",
        "    print('DBSCAN: силуэт не вычисляется (меньше двух кластеров или всё — шум)')\n",
        "\n",
        "# 4.4 Elbow method: покажем WCSS для k=2..9\n",
        "wcss = []  # суммарные внутрикластерные квадраты расстояний\n",
        "for k in range(2, 10):\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    km.fit(X)\n",
        "    wcss.append((k, km.inertia_))  # inertia_ = WCSS\n",
        "print('Elbow WCSS (k, WCSS):', wcss)\n",
        "\n",
        "# 4.5 Silhouette для разных k (2..9)\n",
        "sil_scores = []\n",
        "for k in range(2, 10):\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = km.fit_predict(X)\n",
        "    sil = silhouette_score(X, labels)\n",
        "    sil_scores.append((k, sil))\n",
        "print('Silhouette (k, score):', sil_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задания (Кластеризация)\n",
        "1. Попробуйте увеличить `cluster_std` при генерации данных с 1.0 до 1.8 — как меняются силуэты?  \n",
        "2. В `DBSCAN` варьируйте `eps` (0.5, 0.7, 1.0, 1.2). Как меняется число кластеров и доля шума?  \n",
        "3. Посмотрите WCSS и Silhouette для `k=2..9` и выберите разумное k. Обоснуйте.  \n",
        "4. Замените `AgglomerativeClustering` на метод с `linkage='average'` (через `AgglomerativeClustering(linkage='average', ...)`) и сравните силуэт.  \n",
        "5. Нормализуйте данные (например, `StandardScaler`) перед кластеризацией и сравните результаты для k-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Бустинг и ансамбли: идея и пример оттока\n",
        "**Идея ансамблей:**\n",
        "- **Бэггинг**: много однотипных моделей на разных подвыборках (пример: Random Forest). Снижаем дисперсию.\n",
        "- **Бустинг**: модели строятся последовательно, каждая исправляет ошибки предыдущих (Gradient Boosting, XGBoost, LightGBM, CatBoost). Снижаем смещение.\n",
        "- **Стэкинг**: предсказания базовых моделей идут в метамодель.\n",
        "\n",
        "Ниже — пример бинарной задачи **оттока** на синтетических данных. Сравним базовую Logistic Regression и бустинг (`GradientBoostingClassifier`).\n",
        "\n",
        "Отдельной ячейкой покажу, как аккуратно попробовать XGBoost/LightGBM/CatBoost, **если они установлены** (иначе — пропускаем)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Baseline: Logistic Regression ---\n",
            "Accuracy: 0.9025\n",
            "Precision: 0.8106796116504854\n",
            "Recall: 1.0\n",
            "F1: 0.8954423592493298\n",
            "ROC-AUC: 0.9252910488036801\n",
            "\n",
            "--- Gradient Boosting (sklearn) ---\n",
            "Accuracy: 0.895\n",
            "Precision: 0.8078817733990148\n",
            "Recall: 0.9820359281437125\n",
            "F1: 0.8864864864864865\n",
            "ROC-AUC: 0.9366888540515536\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier  # бустинг из sklearn\n",
        "\n",
        "rng = np.random.default_rng(42)     # генератор случайностей\n",
        "n = 2000                            # количество клиентов\n",
        "\n",
        "# синтетический датасет клиентов с простыми признаками\n",
        "churn_df = pd.DataFrame({\n",
        "    'age': rng.integers(18, 75, n),           # возраст\n",
        "    'balance': rng.integers(0, 300000, n),    # баланс на счёте\n",
        "    'transactions': rng.integers(1, 150, n),  # число операций\n",
        "    'is_active': rng.integers(0, 2, n),       # активен (0/1)\n",
        "    'tenure': rng.integers(1, 15, n)          # стаж (лет)\n",
        "})\n",
        "\n",
        "# формируем вероятность оттока как простую функцию признаков + шум\n",
        "prob = (\n",
        "    0.45 * (churn_df['is_active'] == 0).astype(float) +      # неактивные чаще уходят\n",
        "    0.25 * (churn_df['transactions'] < 10).astype(float) +   # мало операций — риск выше\n",
        "    0.20 * (churn_df['balance'] < 20000).astype(float) +     # низкий баланс — риск\n",
        "    0.10 * (churn_df['tenure'] < 3).astype(float) +          # маленький стаж — риск\n",
        "    rng.random(n) * 0.2                                      # добавим шум\n",
        ")\n",
        "y_churn = (prob > 0.5).astype(int)  # бинаризуем вероятность оттока порогом 0.5\n",
        "\n",
        "Xc = churn_df  # признаки (таблица)\n",
        "yc = y_churn   # целевая переменная (0/1)\n",
        "\n",
        "# делим на train/test со стратификацией по целевой\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    Xc, yc, test_size=0.2, random_state=42, stratify=yc\n",
        ")\n",
        "\n",
        "# БАЗА: логистическая регрессия (часто baseline для бинарных задач)\n",
        "base_lr = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),                           # масштабируем признаки\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))  # обучаем LR\n",
        "])\n",
        "base_lr.fit(Xc_train, yc_train)                   # обучение baseline\n",
        "pred_lr = base_lr.predict(Xc_test)                # предсказания классов\n",
        "proba_lr = base_lr.predict_proba(Xc_test)[:, 1]   # вероятности класса 1\n",
        "\n",
        "print('--- Baseline: Logistic Regression ---')\n",
        "print('Accuracy:', accuracy_score(yc_test, pred_lr))\n",
        "print('Precision:', precision_score(yc_test, pred_lr))\n",
        "print('Recall:', recall_score(yc_test, pred_lr))\n",
        "print('F1:', f1_score(yc_test, pred_lr))\n",
        "print('ROC-AUC:', roc_auc_score(yc_test, proba_lr))\n",
        "\n",
        "# БУСТИНГ: GradientBoostingClassifier (скоро и сравнение)\n",
        "gb = GradientBoostingClassifier(random_state=42)  # стандартные параметры — уже сильная модель\n",
        "gb.fit(Xc_train, yc_train)                        # обучение бустинга\n",
        "pred_gb = gb.predict(Xc_test)                     # предсказания классов\n",
        "proba_gb = gb.predict_proba(Xc_test)[:, 1]        # вероятности класса 1\n",
        "\n",
        "print('\\n--- Gradient Boosting (sklearn) ---')\n",
        "print('Accuracy:', accuracy_score(yc_test, pred_gb))\n",
        "print('Precision:', precision_score(yc_test, pred_gb))\n",
        "print('Recall:', recall_score(yc_test, pred_gb))\n",
        "print('F1:', f1_score(yc_test, pred_gb))\n",
        "print('ROC-AUC:', roc_auc_score(yc_test, proba_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Опционально) XGBoost / LightGBM / CatBoost\n",
        "Ниже — безопасная проверка: если библиотека установлена, модель обучится; если нет, выведем сообщение и продолжим работу ноутбука.  \n",
        "⚠️ Установка при необходимости: `pip install xgboost lightgbm catboost`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- XGBoost ---\n",
            "Accuracy: 0.86\n",
            "Precision: 0.8032786885245902\n",
            "Recall: 0.8802395209580839\n",
            "F1: 0.84\n",
            "ROC-AUC: 0.9287348050679756\n",
            "[LightGBM] [Info] Number of positive: 669, number of negative: 931\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 480\n",
            "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.418125 -> initscore=-0.330475\n",
            "[LightGBM] [Info] Start training from score -0.330475\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "--- LightGBM ---\n",
            "Accuracy: 0.8625\n",
            "Precision: 0.8076923076923077\n",
            "Recall: 0.8802395209580839\n",
            "F1: 0.8424068767908309\n",
            "ROC-AUC: 0.9343116342422451\n",
            "\n",
            "--- CatBoost ---\n",
            "Accuracy: 0.895\n",
            "Precision: 0.8078817733990148\n",
            "Recall: 0.9820359281437125\n",
            "F1: 0.8864864864864865\n",
            "ROC-AUC: 0.9407622523193955\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from xgboost import XGBClassifier  # импортируем класс XGBoost\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=300,        # число деревьев\n",
        "        max_depth=4,            # глубина деревьев\n",
        "        learning_rate=0.1,      # скорость обучения (шаг градиента)\n",
        "        subsample=0.8,          # доля выборки для каждого дерева (bagging по объектам)\n",
        "        colsample_bytree=0.8,   # доля фичей для каждого дерева (bagging по признакам)\n",
        "        random_state=42,        # воспроизводимость\n",
        "        eval_metric='logloss'   # метрика обучения внутри XGB\n",
        "    )\n",
        "    xgb.fit(Xc_train, yc_train)               # обучение XGBoost\n",
        "    pred_xgb = xgb.predict(Xc_test)           # предсказания классов\n",
        "    proba_xgb = xgb.predict_proba(Xc_test)[:, 1]  # вероятности класса 1\n",
        "    print('--- XGBoost ---')\n",
        "    print('Accuracy:', accuracy_score(yc_test, pred_xgb))\n",
        "    print('Precision:', precision_score(yc_test, pred_xgb))\n",
        "    print('Recall:', recall_score(yc_test, pred_xgb))\n",
        "    print('F1:', f1_score(yc_test, pred_xgb))\n",
        "    print('ROC-AUC:', roc_auc_score(yc_test, proba_xgb))\n",
        "except Exception as e:\n",
        "    print('XGBoost недоступен или не установлен:', e)\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier  # импорт LightGBM\n",
        "    lgbm = LGBMClassifier(random_state=42)  # базовая конфигурация\n",
        "    lgbm.fit(Xc_train, yc_train)           # обучение LightGBM\n",
        "    pred_lgbm = lgbm.predict(Xc_test)      # предсказания классов\n",
        "    proba_lgbm = lgbm.predict_proba(Xc_test)[:, 1]  # вероятности класса 1\n",
        "    print('\\n--- LightGBM ---')\n",
        "    print('Accuracy:', accuracy_score(yc_test, pred_lgbm))\n",
        "    print('Precision:', precision_score(yc_test, pred_lgbm))\n",
        "    print('Recall:', recall_score(yc_test, pred_lgbm))\n",
        "    print('F1:', f1_score(yc_test, pred_lgbm))\n",
        "    print('ROC-AUC:', roc_auc_score(yc_test, proba_lgbm))\n",
        "except Exception as e:\n",
        "    print('LightGBM недоступен или не установлен:', e)\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier  # импорт CatBoost\n",
        "    cat = CatBoostClassifier(verbose=0, random_state=42)  # отключаем подробные логи\n",
        "    cat.fit(Xc_train, yc_train)              # обучение CatBoost\n",
        "    pred_cat = cat.predict(Xc_test)          # предсказания классов (массив строк '0'/'1' или чисел)\n",
        "    # у catboost predict может вернуть строки; приведём к int\n",
        "    pred_cat = pred_cat.astype(int).ravel()  # преобразуем к int и выровняем форму\n",
        "    proba_cat = cat.predict_proba(Xc_test)[:, 1]  # вероятности класса 1\n",
        "    print('\\n--- CatBoost ---')\n",
        "    print('Accuracy:', accuracy_score(yc_test, pred_cat))\n",
        "    print('Precision:', precision_score(yc_test, pred_cat))\n",
        "    print('Recall:', recall_score(yc_test, pred_cat))\n",
        "    print('F1:', f1_score(yc_test, pred_cat))\n",
        "    print('ROC-AUC:', roc_auc_score(yc_test, proba_cat))\n",
        "except Exception as e:\n",
        "    print('CatBoost недоступен или не установлен:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задания (Бустинг и ансамбли)\n",
        "1. В `GradientBoostingClassifier` измените `n_estimators` (100, 300, 600) и `learning_rate` (0.05, 0.1, 0.2). Как меняется ROC-AUC?  \n",
        "2. Для XGBoost (если установлен) попробуйте `max_depth` = 3 и 6; сравните Recall и Precision.  \n",
        "3. Постройте важности признаков (feature importance) для `RandomForest` и (если есть) XGBoost. Совпадают ли топ-фичи?  \n",
        "4. Измените генерацию `prob` так, чтобы более важным стал `balance` и менее — `is_active`. Как изменятся метрики моделей?  \n",
        "5. Попробуйте `class_weight='balanced'` в Logistic Regression на задаче оттока и сравните с бустингом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Итоговые упражнения (сквозные)\n",
        "1. Возьмите один свой датасет (табличный), пройдите весь цикл: EDA → препроцессинг → train/test → модель (регрессия или классификация) → метрики → интерпретация.  \n",
        "2. Сравните минимум три модели на одной задаче (например, LR, RF, GB). Сформируйте табличку с метриками.  \n",
        "3. Для кластеризации: примените k-means и DBSCAN к тем же данным (после нормализации) — попробуйте объяснить бизнес-смысл кластеров.  \n",
        "4. Напишите небольшой отчёт (1–2 страницы): цель, данные, подход, результаты, ограничения, планы улучшений.  \n",
        "5. Вынесите лучший пайплайн в функцию/класс, чтобы переиспользовать на других данных."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
